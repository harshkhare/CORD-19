{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/kaggle/input/CORD-19-research-challenge/'\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "len(all_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Abstract\n",
    "            for entry in content['abstract']:\n",
    "                self.abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append(entry['text'])\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0 of 60596\n",
      "Processing index: 6059 of 60596\n",
      "Processing index: 12118 of 60596\n",
      "Processing index: 18177 of 60596\n",
      "Processing index: 24236 of 60596\n",
      "Processing index: 30295 of 60596\n",
      "Processing index: 36354 of 60596\n",
      "Processing index: 42413 of 60596\n",
      "Processing index: 48472 of 60596\n",
      "Processing index: 54531 of 60596\n",
      "Processing index: 60590 of 60596\n"
     ]
    }
   ],
   "source": [
    "dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "for idx, entry in enumerate(all_json):\n",
    "    if idx % (len(all_json) // 10) == 0:\n",
    "        print(f'Processing index: {idx} of {len(all_json)}')\n",
    "    \n",
    "    try:\n",
    "        content = FileReader(entry)\n",
    "    except Exception as e:\n",
    "        continue  # invalid paper format, skip\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "    \n",
    "    # also create a column for the summary of abstract to be used in a plot\n",
    "    if len(content.abstract) == 0: \n",
    "        # no abstract provided\n",
    "        dict_['abstract_summary'].append(\"Not provided.\")\n",
    "    elif len(content.abstract.split(' ')) > 100:\n",
    "        # abstract provided is too long for plot, take first 300 words append with ...\n",
    "        info = content.abstract.split(' ')[:100]\n",
    "        summary = get_breaks(' '.join(info), 40)\n",
    "        dict_['abstract_summary'].append(summary + \"...\")\n",
    "    else:\n",
    "        # abstract is short enough\n",
    "        summary = get_breaks(content.abstract, 40)\n",
    "        dict_['abstract_summary'].append(summary)\n",
    "        \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    \n",
    "    try:\n",
    "        # if more than one author\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "        if len(authors) > 2:\n",
    "            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n",
    "            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n",
    "        else:\n",
    "            # authors will fit in plot\n",
    "            dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if only one author - or Null valie\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "    \n",
    "    # add the title information, add breaks when needed\n",
    "    try:\n",
    "        title = get_breaks(meta_data['title'].values[0], 40)\n",
    "        dict_['title'].append(title)\n",
    "    # if title was not provided\n",
    "    except Exception as e:\n",
    "        dict_['title'].append(meta_data['title'].values[0])\n",
    "    \n",
    "    # add the journal information\n",
    "    dict_['journal'].append(meta_data['journal'].values[0])\n",
    "    \n",
    "    # add doi\n",
    "    dict_['doi'].append(meta_data['doi'].values[0])\n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine abstract and body text\n",
    "df_covid['abstract_body_text']   = df_covid['abstract']+' '+df_covid['body_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process keywords from external databases\n",
    "path = 'externaldataset/*.txt'\n",
    "keyword_files = glob.glob(path)\n",
    "all_keyword =[]\n",
    "for f in keyword_files:\n",
    "    file_object  = open(f, 'r')\n",
    "    keyword = file_object.read()\n",
    "    keyword_arr = keyword.split('\\n')\n",
    "    keyword_arr_clean = [i.lower().replace('/', ' ').replace('^+^', '').replace(':','').replace('-','') for i in keyword_arr]\n",
    "    all_keyword.append(keyword_arr_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15814"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = [item.strip().lower() for sublist in all_keyword for item in sublist if item]\n",
    "keywords_uni = set(keywords)\n",
    "len(keywords_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match keywords\n",
    "import re\n",
    "def match_keyword(text):\n",
    "    return [dis for dis in keywords_uni if dis in text.lower()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save keyword matches to files of 500 documents each. These files will be used in 3.lda_visualization.ipynb.\n",
    "# This process takes a long time. Change the value of count. You can start from where you had left.\n",
    "import time\n",
    "import swifter\n",
    "n = 500  #chunk row size\n",
    "list_df = [df_covid[i:i+n] for i in range(0,df_covid.shape[0],n)]\n",
    "count=60 # Choose your count, you can start from where you left.\n",
    "for i in range(count, len(list_df)):\n",
    "    print('processing..', count)\n",
    "    df = list_df[i]\n",
    "    df['keyword_match']=df.swifter.apply(lambda row: match_keyword(row['abstract_body_text']),axis=1)\n",
    "    df[['paper_id','keyword_match']].to_csv('keyword_match_data/df_'+str(count)+'.csv', index=False)\n",
    "    count=count+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
