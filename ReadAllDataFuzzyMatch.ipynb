{
  "cells": [
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "DhUbpq5t6Sto"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import glob\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CiAYVzJr6Sts"
      },
      "cell_type": "code",
      "source": [
        "root_path = '/kaggle/input/CORD-19-research-challenge/'\n",
        "metadata_path = f'{root_path}/metadata.csv'\n",
        "meta_df = pd.read_csv(metadata_path, dtype={\n",
        "    'pubmed_id': str,\n",
        "    'Microsoft Academic Paper ID': str, \n",
        "    'doi': str\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lgRyrdvh6Stt",
        "outputId": "249e1470-6d63-4aea-cb18-e7ae77a4b01f"
      },
      "cell_type": "code",
      "source": [
        "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
        "len(all_json)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "60596"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gsBbruTN6Stu"
      },
      "cell_type": "code",
      "source": [
        "class FileReader:\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path) as file:\n",
        "            content = json.load(file)\n",
        "            self.paper_id = content['paper_id']\n",
        "            self.abstract = []\n",
        "            self.body_text = []\n",
        "            # Abstract\n",
        "            for entry in content['abstract']:\n",
        "                self.abstract.append(entry['text'])\n",
        "            # Body text\n",
        "            for entry in content['body_text']:\n",
        "                self.body_text.append(entry['text'])\n",
        "            self.abstract = '\\n'.join(self.abstract)\n",
        "            self.body_text = '\\n'.join(self.body_text)\n",
        "    def __repr__(self):\n",
        "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cVtDPCM76Stu"
      },
      "cell_type": "code",
      "source": [
        "def get_breaks(content, length):\n",
        "    data = \"\"\n",
        "    words = content.split(' ')\n",
        "    total_chars = 0\n",
        "\n",
        "    # add break every length characters\n",
        "    for i in range(len(words)):\n",
        "        total_chars += len(words[i])\n",
        "        if total_chars > length:\n",
        "            data = data + \"<br>\" + words[i]\n",
        "            total_chars = 0\n",
        "        else:\n",
        "            data = data + \" \" + words[i]\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5ByPKaTF6Stv",
        "outputId": "70c34cb8-6a5a-4823-8de2-ca9f3dbea353"
      },
      "cell_type": "code",
      "source": [
        "dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
        "for idx, entry in enumerate(all_json):\n",
        "    if idx % (len(all_json) // 10) == 0:\n",
        "        print(f'Processing index: {idx} of {len(all_json)}')\n",
        "    \n",
        "    try:\n",
        "        content = FileReader(entry)\n",
        "    except Exception as e:\n",
        "        continue  # invalid paper format, skip\n",
        "    \n",
        "    # get metadata information\n",
        "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
        "    # no metadata, skip this paper\n",
        "    if len(meta_data) == 0:\n",
        "        continue\n",
        "    \n",
        "    dict_['abstract'].append(content.abstract)\n",
        "    dict_['paper_id'].append(content.paper_id)\n",
        "    dict_['body_text'].append(content.body_text)\n",
        "    \n",
        "    # also create a column for the summary of abstract to be used in a plot\n",
        "    if len(content.abstract) == 0: \n",
        "        # no abstract provided\n",
        "        dict_['abstract_summary'].append(\"Not provided.\")\n",
        "    elif len(content.abstract.split(' ')) > 100:\n",
        "        # abstract provided is too long for plot, take first 300 words append with ...\n",
        "        info = content.abstract.split(' ')[:100]\n",
        "        summary = get_breaks(' '.join(info), 40)\n",
        "        dict_['abstract_summary'].append(summary + \"...\")\n",
        "    else:\n",
        "        # abstract is short enough\n",
        "        summary = get_breaks(content.abstract, 40)\n",
        "        dict_['abstract_summary'].append(summary)\n",
        "        \n",
        "    # get metadata information\n",
        "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
        "    \n",
        "    try:\n",
        "        # if more than one author\n",
        "        authors = meta_data['authors'].values[0].split(';')\n",
        "        if len(authors) > 2:\n",
        "            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n",
        "            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n",
        "        else:\n",
        "            # authors will fit in plot\n",
        "            dict_['authors'].append(\". \".join(authors))\n",
        "    except Exception as e:\n",
        "        # if only one author - or Null valie\n",
        "        dict_['authors'].append(meta_data['authors'].values[0])\n",
        "    \n",
        "    # add the title information, add breaks when needed\n",
        "    try:\n",
        "        title = get_breaks(meta_data['title'].values[0], 40)\n",
        "        dict_['title'].append(title)\n",
        "    # if title was not provided\n",
        "    except Exception as e:\n",
        "        dict_['title'].append(meta_data['title'].values[0])\n",
        "    \n",
        "    # add the journal information\n",
        "    dict_['journal'].append(meta_data['journal'].values[0])\n",
        "    \n",
        "    # add doi\n",
        "    dict_['doi'].append(meta_data['doi'].values[0])\n",
        "    \n",
        "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing index: 0 of 60596\nProcessing index: 6059 of 60596\nProcessing index: 12118 of 60596\nProcessing index: 18177 of 60596\nProcessing index: 24236 of 60596\nProcessing index: 30295 of 60596\nProcessing index: 36354 of 60596\nProcessing index: 42413 of 60596\nProcessing index: 48472 of 60596\nProcessing index: 54531 of 60596\nProcessing index: 60590 of 60596\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "czpS8yAR6Stw"
      },
      "cell_type": "code",
      "source": [
        "df_covid['abstract_body_text']   = df_covid['abstract']+' '+df_covid['body_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bwvopOwO6Stx"
      },
      "cell_type": "code",
      "source": [
        "path = '/kaggle/input/externaldataset/*.txt'\n",
        "#, path+'10_keggMedicus_network.list.txt'\n",
        "#,path+'09_keggMedicus_drug.list.txt'\n",
        "# path='/kaggle/input/externaldataset/'\n",
        "keyword_files = glob.glob(path)\n",
        "# keyword_files =[path+'08_new_keggMedicus_disease.list.txt'] #2537 all kegg data= 14K, all data 15K\n",
        "all_keyword =[]\n",
        "for f in keyword_files:\n",
        "    file_object  = open(f, 'r')\n",
        "    keyword = file_object.read()\n",
        "    keyword_arr = keyword.split('\\n')\n",
        "    keyword_arr_clean = [i.lower().replace('/', ' ').replace('^+^', '').replace(':','').replace('-','') for i in keyword_arr]\n",
        "    all_keyword.append(keyword_arr_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qVUd4TC26Sty",
        "outputId": "14ee49ff-5447-4d18-8d3e-25da6d4effee"
      },
      "cell_type": "code",
      "source": [
        "keywords = [item.strip().lower() for sublist in all_keyword for item in sublist if item]\n",
        "keywords_uni = set(keywords)\n",
        "len(keywords_uni)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "15814"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "Pu-hK1h76Sty"
      },
      "cell_type": "markdown",
      "source": [
        "#new code"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "83esX2nB6Stz"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "def match_keyword(text):\n",
        "    return [dis for dis in keywords_uni if dis in text.lower()] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Qjm_E06-6St0"
      },
      "cell_type": "code",
      "source": [
        "# !pip install swifter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Fvjh6TrH6St0",
        "outputId": "c53209c1-4214-4407-e8c8-ca1315c751f0"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import swifter\n",
        "n = 500  #chunk row size\n",
        "list_df = [df_covid[i:i+n] for i in range(0,df_covid.shape[0],n)]\n",
        "count=3\n",
        "for i in range(count, len(list_df)):\n",
        "    print('processing..', count)\n",
        "    df = list_df[i]\n",
        "    df['keyword_match']=df.swifter.apply(lambda row: match_keyword(row['abstract_body_text']),axis=1)\n",
        "    df.to_csv('df_'+str(count)+'.csv')\n",
        "    count=count+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "processing.. 3\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'swifter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b7746ee064d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processing..'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keyword_match'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswifter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmatch_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract_body_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'swifter'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "i0UZ0ZiI6St1"
      },
      "cell_type": "code",
      "source": [
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "coRACTke6St1"
      },
      "cell_type": "code",
      "source": [
        "!pip install scispacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pKsPqRiF6St1"
      },
      "cell_type": "code",
      "source": [
        "import scispacy\n",
        "import spacy\n",
        "import en_core_sci_lg\n",
        "from spacy import displacy\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "from scispacy.umls_linking import UmlsEntityLinker\n",
        "\n",
        "# nlp = spacy.load(\"en_core_sci_sm\")\n",
        "nlp = spacy.load(\"en_core_sci_lg\")\n",
        "# nlp = en_core_sci_lg.load(disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "nlp.max_length = 2000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lIEsfmX86St1"
      },
      "cell_type": "code",
      "source": [
        "text = \"Red mice was done. Calf diarrhea is a commonly reported disease in young animals, and still a major cause of productivity and economic loss to cattle producers worldwide. In the report of the 2007 National Animal Health Monitoring System for U.S. dairy, half of the deaths among unweaned calves was attributed to diarrhea. Multiple pathogens are known or postulated to cause or contribute to calf diarrhea development. Other factors including both the environment and management practices influence disease severity or outcomes. The multifactorial nature of calf diarrhea makes this disease hard to control effectively in modern cow-calf operations. The purpose of this review is to provide a better understanding of a) the ecology and pathogenesis of well-known and potential bovine enteric pathogens implicated in calf diarrhea, b) describe diagnostic tests used to detect various enteric pathogens along with their pros and cons, and c) propose improved intervention strategies for treating calf diarrhea.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "a_-JFbYc6St2"
      },
      "cell_type": "code",
      "source": [
        "# def my_tokenizer_ent(sentence):\n",
        "#     return set([str(word).lower() for word in nlp(text).ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1d58_IZV6St2"
      },
      "cell_type": "code",
      "source": [
        "# data_vect  = pd.DataFrame()\n",
        "df_covid['abstract_body_text_nlp'] = df_covid['abstract_body_text'].apply(lambda x : nlp(x).ents)\n",
        "# token_ent  = set([str(word).lower() for word in nlp(text).ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EdYNGfXi6St2"
      },
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import process\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "def fuzzy_match(x, choices, scorer, cutoff):\n",
        "    match = process.extractOne(x['entities'], \n",
        "                               choices=choices, \n",
        "                               scorer=scorer, \n",
        "                               score_cutoff=cutoff)\n",
        "    if match:\n",
        "        return match[0]\n",
        "    \n",
        "def get_keyword_match(data_vect):\n",
        "    return data_vect.apply(fuzzy_match, args=(list(keywords_uni), fuzz.token_set_ratio, 100), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Riztarg56St2"
      },
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XTtGMLfO6St2"
      },
      "cell_type": "code",
      "source": [
        "data_vect[pd.notnull(data_vect['FuzzyMatch'])]\n",
        "# data_vect[pd.notnull(data_vect['FuzzyMatch'])].to_csv('entities_match2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "prF5aJNo6St2"
      },
      "cell_type": "code",
      "source": [
        "def my_tokenizer(sentence):\n",
        "    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tsENDE-K6St3"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer='word',                   # minimum reqd occurences of a word \n",
        "                              stop_words='english',             # remove stop words\n",
        "                              lowercase=True,                   # convert all words to lowercase\n",
        "                              token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
        "                              tokenizer = my_tokenizer,\n",
        "                              min_df=2         # max number of uniq words\n",
        "                             )\n",
        "data_vectorized = vectorizer.fit_transform(df_covid['abstract_body_text'])\n",
        "data_vectorized_mod = {k:v for k, v in (vectorizer.vocabulary_.items()) if not k.isdigit()}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}